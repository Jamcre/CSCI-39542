"""
Name:  James Michael Crespo
Email: james.crespo64@myhunter.cuny.edu
Resources: the internet
"""
import pickle
import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures


def import_data(csv_file):
    """
    The data in file is read into a DataFrame, and yes is the 'USINFO' column of your dataset.
    The xes can be generated by looping through the dataset or using the index as a column
    (e.g. df.index.to_series() adds an additional column, based on the index).
    The resulting DataFrame is returned.

    :param csv_file: the name of a CSV file time series data for a commodity from FRED.
    """
    df = pd.read_csv(csv_file)
    df['units'] = df.index
    return df


def split_data(df, y_col_name, test_size=0.25, random_state=21):
    """
    Returns data split in 4 subsets, from train_test_split: x_train, x_test, y_train, and y_test.
    Where units is the "x" column and the input parameter, y_col_name is the "y" column.

    :param df: a DataFrame containing with a column 'units'.
    :param y_col_name: the name of the column of the dependent variable.
    :param test_size: 0 to 1 value, show proportion of data set to use for training. Default=0.25
    :param random_state: Used as a seed to the randomization. Default=21
    """
    x_data = df['units']
    y_data = df[y_col_name]
    x_train, x_test, y_train, y_test = train_test_split(
        x_data, y_data, test_size=test_size, random_state=random_state)
    return x_train, x_test, y_train, y_test


def fit_poly(xes, yes, epsilon=100, verbose=False):
    """
    It returns the smallest integer degree >= 1 for which the model yields a MSE of less than
    the specified epsilon and the coefficients as a vector for df["units"] and df[y_col].
    If it does not find a model with an error less than epsilon by degree 5, returns None.
    When fitting the linear regression model, the fit_intercept=False.

    :param xes: a DataFrame that includes the column 'units'.
    :param yes: a series of the same length as xes.
    :param epsilon: the size of the sample. Default = 100.
    :param verbose: when True, prints out the MSE cost for each degree tried
    (in format: f'MSE cost for deg {deg} poly model: {error:.3f}' for degrees 1, 2, ...,
    until the error is below epsilon, see example below). Default=False.
    """
    # Iterate from deg=1 to deg=5
    for deg in range(1, 6):
        x_poly = PolynomialFeatures(deg).fit_transform(xes)
        model = LinearRegression(fit_intercept=False).fit(x_poly, yes)
        error = mean_squared_error(yes, model.predict(x_poly))
        if verbose:
            print(f'MSE cost for deg {deg} poly model: {error:.3f}')
        if error < epsilon:
            return deg
    return None


def fit_model(xes, yes, poly_deg=2, reg="lasso"):
    """
    This function fits a model with polynomial features using Lasso or Ridge regression with
    cross validation: Apply PolynomialFeatures to the xes with degree equal to poly_deg.
    If reg = ridge, use RidgeCV to instantiate and fit a model to the polynomial features and yes.
    Otherwise, use LassoCV to to instantiate and fit the model.
    Returns the model as serialized object (i.e. a pickled object).

    :param xes: a series of numeric values.
    :param yes: a series of numeric values.
    :param poly_deg: the degree of the polynomial features to be created. Default=2.
    :param reg: The type of regularization used: ridge or lasso. Default='lasso'.
    """
    # Create polynomial features
    poly_features = PolynomialFeatures(degree=poly_deg)
    x_poly = poly_features.fit_transform(xes.reshape(-1, 1))

    # Choose regularization method
    if reg == "ridge":
        model = RidgeCV(cv=5)
    else:
        model = LassoCV(cv=5)

    # Fit model to data
    model.fit(x_poly, yes)

    # Serialize and return model
    return pickle.dumps(model)


def predict_using_trained_model(mod_pkl, poly_xes, yes):
    """
    Computes and returns the mean squared error and r2 score between the values predicted,
    by the model (mod on x) and the actual values (y).
    Note: sklearn.metrics contains two funcs that may be of use: mean_squared_error and r2_score.

    :param mod_pkl: a trained model for the data, stored in pickle format.
    :param poly_xes: an array or DataFrame of numeric columns with no null values.
    :param yes: an array or DataFrame of numeric columns with no null values.
    """
    # load model
    model = pickle.loads(mod_pkl)
    # predict using mod_pkl on poly_xes
    y_pred = model.predict(poly_xes)
    # compare prediction and actual
    mse = mean_squared_error(yes, y_pred)
    r2_val = r2_score(yes, y_pred)

    # Return mean squared error and r2 score as tuple
    return mse, r2_val


def main():
    """tests"""
    csv_file = "program7/fred_cpi_eggs_oil_cars_INDEXED.csv"
    df = import_data(csv_file)
    print('The DataFrame:')
    print(df)

    y_col_name = "CPI"
    print(f'For the x column = "units", y_col = {y_col_name}')
    print(df[["units", y_col_name]])
    x_train_cpi, x_test_cpi, y_train_cpi, y_test_cpi = split_data(
        df, y_col_name)

    print('\nReturned sets of lengths:')
    print(f"x_train_cpi: {len(x_train_cpi)}, x_test_cpi: {len(x_test_cpi)}")
    print(f"y_train_cpi: {len(y_train_cpi)}, y_test_cpi: {len(y_test_cpi)}")

    # eps = 5
    # print(f'Finding the poly degree for training data with epsilon = {eps}:')
    # deg = fit_poly(x_train_cpi.to_frame(), y_train_cpi,
    #                epsilon=eps, verbose=True)
    # print(f'For epsilon = {eps}, poly has degree {deg}.')


if __name__ == "__main__":
    main()
